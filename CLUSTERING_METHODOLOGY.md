# K-meansèšç±»æ–¹æ³•è¯¦ç»†æµç¨‹è¯´æ˜

æœ¬æ–‡æ¡£è¯¦ç»†è§£é‡Šæ–¹æ³•10ï¼ˆK-meansèšç±»åˆ†ç±»å™¨ï¼‰çš„å®Œæ•´æŠ€æœ¯æµç¨‹ï¼Œä»æ•°æ®æ¸…æ´—åˆ°æ ‡ç­¾ç”Ÿæˆã€‚

---

## ğŸ“‹ å®Œæ•´æµç¨‹æ¦‚è§ˆ

```
åŸå§‹æ•°æ® (40,762æ ·æœ¬)
    â†“
[æ­¥éª¤1] æ•°æ®æ¸…æ´— - è¿‡æ»¤å¼‚å¸¸å€¼
    â†“
æ¸…æ´—æ•°æ® (40,526æ ·æœ¬, 99.4%)
    â†“
[æ­¥éª¤2] ç‰¹å¾æå– - 10ç»´ç‰¹å¾å‘é‡
    â†“
ç‰¹å¾çŸ©é˜µ (40,526 Ã— 10)
    â†“
[æ­¥éª¤3] ç‰¹å¾æ ‡å‡†åŒ– - StandardScaler
    â†“
æ ‡å‡†åŒ–ç‰¹å¾çŸ©é˜µ
    â†“
[æ­¥éª¤4] Kå€¼é€‰æ‹© - æµ‹è¯•K=4~8
    â†“
æœ€ä¼˜K=8 (Silhouette=0.5384, Balance=1.056)
    â†“
[æ­¥éª¤5] K-meansèšç±» - ç”Ÿæˆ8ä¸ªcluster
    â†“
Cluster 0~7 (æ— åºï¼Œç”±ç®—æ³•éšæœºåˆ†é…)
    â†“
[æ­¥éª¤6] Clusteråˆ†æ - è®¡ç®—æ¯ä¸ªclusterçš„ç»Ÿè®¡ç‰¹å¾
    â†“
[æ­¥éª¤7] Clusteræ’åº - æŒ‰å¹³å‡countsä»å°åˆ°å¤§æ’åº
    â†“
[æ­¥éª¤8] æ ‡ç­¾æ˜ å°„ - Cluster ID â†’ Label 0~7
    â†“
æœ€ç»ˆæ ‡ç­¾æ•°æ®é›† (40,526æ ·æœ¬å¸¦label)
```

---

## ğŸ” è¯¦ç»†æ­¥éª¤è§£æ

### æ­¥éª¤1ï¸âƒ£: æ•°æ®æ¸…æ´— - è¿‡æ»¤å¼‚å¸¸å€¼

**ç›®çš„**: ç§»é™¤å™ªå£°æ•°æ®ï¼Œé¿å…æç«¯å€¼å½±å“èšç±»æ•ˆæœ

**ä»£ç **:
```python
# åŸå§‹æ•°æ®
df = pd.read_json('data/merged_with_labels_and_counts.jsonl', lines=True)
# æ€»æ ·æœ¬: 40,762
# countsèŒƒå›´: 1 - 943

# è¿‡æ»¤å¼‚å¸¸å€¼
df_clean = df[df['counts'] <= 130].copy()
anomalies = df[df['counts'] > 130].copy()

# ç»“æœ:
# ä¿ç•™: 40,526 æ ·æœ¬ (99.4%)
# ç§»é™¤: 236 æ ·æœ¬ (0.6%)
# æ–°countsèŒƒå›´: 1 - 130
```

**ä¸ºä»€ä¹ˆé€‰æ‹©130ä½œä¸ºé˜ˆå€¼ï¼Ÿ**
- æ•°æ®åˆ†å¸ƒåˆ†ææ˜¾ç¤ºcounts>130çš„æ ·æœ¬æå°‘ä¸”åˆ†æ•£
- è¿™äº›æ ·æœ¬çš„countså€¼èŒƒå›´æå¹¿ï¼ˆ133-943ï¼‰
- å®ƒä»¬ä¼šåœ¨èšç±»ä¸­å½¢æˆç¦»ç¾¤ç‚¹ï¼Œå½±å“æ•´ä½“æ•ˆæœ

**ç§»é™¤å‰åå¯¹æ¯”**:
```
ç§»é™¤å‰:
  Mean counts: 11.2
  Median counts: 7.0
  Max counts: 943 (æç«¯å¼‚å¸¸å€¼)

ç§»é™¤å:
  Mean counts: 11.0
  Median counts: 7.0
  Max counts: 130 (æ›´åˆç†çš„ä¸Šç•Œ)
```

---

### æ­¥éª¤2ï¸âƒ£: ç‰¹å¾æå– - æ„å»º10ç»´ç‰¹å¾å‘é‡

**ç›®çš„**: ä»æ–‡æœ¬ä¸­æå–èƒ½å¤Ÿä»£è¡¨ä»»åŠ¡ç±»å‹å’Œå¤æ‚åº¦çš„æ•°å€¼ç‰¹å¾

**ç‰¹å¾åˆ—è¡¨**:

| ç‰¹å¾åç§° | ç±»å‹ | æå–æ–¹æ³• | å«ä¹‰ |
|---------|------|---------|------|
| `counts` | è¿ç»­å€¼ | ç›´æ¥ä½¿ç”¨ | æ€ç»´é“¾å¤æ‚åº¦ï¼ˆæ ¸å¿ƒç‰¹å¾ï¼‰ |
| `word_count` | è¿ç»­å€¼ | `text.split().len()` | æŸ¥è¯¢é•¿åº¦ |
| `char_count` | è¿ç»­å€¼ | `len(text)` | å­—ç¬¦æ•°é‡ |
| `question_marks` | è¿ç»­å€¼ | `text.count('?')` | é—®é¢˜æ•°é‡ |
| `has_explain` | äºŒå€¼ (0/1) | æ­£åˆ™åŒ¹é… | æ˜¯å¦åŒ…å«explain/describe/analyze/compare |
| `has_creative` | äºŒå€¼ (0/1) | æ­£åˆ™åŒ¹é… | æ˜¯å¦åŒ…å«write/create/generate/story |
| `has_calculate` | äºŒå€¼ (0/1) | æ­£åˆ™åŒ¹é… | æ˜¯å¦åŒ…å«calculate/compute/solve |
| `has_list` | äºŒå€¼ (0/1) | æ­£åˆ™åŒ¹é… | æ˜¯å¦åŒ…å«list/enumerate |
| `has_why` | äºŒå€¼ (0/1) | æ­£åˆ™åŒ¹é… | æ˜¯å¦åŒ…å«why |
| `has_how` | äºŒå€¼ (0/1) | æ­£åˆ™åŒ¹é… | æ˜¯å¦åŒ…å«how |

**ä»£ç ç¤ºä¾‹**:
```python
df_clean['text'] = df_clean['instruction'].fillna('') + ' ' + df_clean['input'].fillna('')
df_clean['word_count'] = df_clean['text'].str.split().str.len()
df_clean['char_count'] = df_clean['text'].str.len()
df_clean['question_marks'] = df_clean['text'].str.count(r'\?')
df_clean['has_explain'] = df_clean['text'].str.lower().str.contains('explain|describe|analyze|compare').astype(int)
df_clean['has_creative'] = df_clean['text'].str.lower().str.contains('write|create|generate|story').astype(int)
# ... (å…¶ä»–ç‰¹å¾åŒç†)

# æ„å»ºç‰¹å¾çŸ©é˜µ
feature_cols = ['counts', 'word_count', 'char_count', 'question_marks',
                'has_explain', 'has_creative', 'has_calculate', 'has_list',
                'has_why', 'has_how']
X = df_clean[feature_cols].values  # Shape: (40526, 10)
```

**ç‰¹å¾ç¤ºä¾‹**:

| æŸ¥è¯¢ | counts | word_count | has_explain | has_creative | has_calculate |
|------|--------|-----------|-------------|--------------|---------------|
| "Explain how photosynthesis works" | 19 | 4 | 1 | 0 | 0 |
| "Write a short story about dragons" | 12 | 6 | 0 | 1 | 0 |
| "Calculate the area of a circle" | 8 | 6 | 0 | 0 | 1 |

---

### æ­¥éª¤3ï¸âƒ£: ç‰¹å¾æ ‡å‡†åŒ– - StandardScaler

**ç›®çš„**: æ¶ˆé™¤ä¸åŒç‰¹å¾çš„é‡çº²å·®å¼‚ï¼Œè®©K-meansèƒ½å¤Ÿå…¬å¹³åœ°æ¯”è¾ƒæ‰€æœ‰ç‰¹å¾

**ä¸ºä»€ä¹ˆéœ€è¦æ ‡å‡†åŒ–ï¼Ÿ**
```python
# æ ‡å‡†åŒ–å‰çš„ç‰¹å¾èŒƒå›´:
counts:          1 - 130      (èŒƒå›´å¾ˆå¤§)
word_count:      1 - 200      (èŒƒå›´å¾ˆå¤§)
char_count:      5 - 2000     (èŒƒå›´å·¨å¤§ï¼)
question_marks:  0 - 5        (èŒƒå›´å¾ˆå°)
has_explain:     0 - 1        (äºŒå€¼ç‰¹å¾)
```

å¦‚æœä¸æ ‡å‡†åŒ–ï¼Œ`char_count`ä¼šä¸»å¯¼èšç±»ç»“æœï¼ˆå› ä¸ºæ•°å€¼èŒƒå›´å¤§ï¼‰ï¼Œè€ŒäºŒå€¼ç‰¹å¾ï¼ˆå¦‚`has_explain`ï¼‰å‡ ä¹æ²¡æœ‰å½±å“åŠ›ã€‚

**StandardScaleråŸç†**:
```python
# å¯¹æ¯ä¸ªç‰¹å¾è¿›è¡Œæ ‡å‡†åŒ–
X_scaled[i, j] = (X[i, j] - mean[j]) / std[j]

# ç»“æœ: æ¯ä¸ªç‰¹å¾çš„å‡å€¼=0, æ ‡å‡†å·®=1
```

**ä»£ç **:
```python
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)  # Shape: (40526, 10)

# æ ‡å‡†åŒ–åï¼Œæ‰€æœ‰ç‰¹å¾éƒ½åœ¨ç›¸ä¼¼çš„èŒƒå›´å†…ï¼ˆé€šå¸¸-3åˆ°+3ï¼‰
```

**æ ‡å‡†åŒ–å‰åå¯¹æ¯”**:
```
åŸå§‹ç‰¹å¾:
  counts:     [1, 2, 5, 12, 130, ...]
  word_count: [3, 8, 15, 50, 200, ...]

æ ‡å‡†åŒ–å:
  counts:     [-1.2, -1.1, -0.8, 0.1, 2.5, ...]
  word_count: [-1.0, -0.5, 0.2, 1.5, 3.0, ...]
```

---

### æ­¥éª¤4ï¸âƒ£: Kå€¼é€‰æ‹© - æµ‹è¯•å¤šä¸ªKå€¼

**ç›®çš„**: æ‰¾åˆ°æœ€ä¼˜çš„èšç±»æ•°é‡ï¼Œå¹³è¡¡èšç±»è´¨é‡å’Œç±»åˆ«å‡è¡¡æ€§

**æµ‹è¯•èŒƒå›´**: K = 4, 5, 6, 7, 8

**è¯„ä»·æŒ‡æ ‡**:

1. **Silhouette Score (è½®å»“ç³»æ•°)** - è¡¡é‡èšç±»è´¨é‡
   - èŒƒå›´: [-1, 1]
   - è¶Šæ¥è¿‘1è¶Šå¥½ï¼Œè¡¨ç¤ºæ ·æœ¬ä¸æ‰€å±clusterç›¸ä¼¼ï¼Œä¸å…¶ä»–clusterä¸åŒ
   - å…¬å¼: `silhouette = (b - a) / max(a, b)`
     - `a` = æ ·æœ¬ä¸åŒclusterå†…å…¶ä»–æ ·æœ¬çš„å¹³å‡è·ç¦»
     - `b` = æ ·æœ¬ä¸æœ€è¿‘çš„å…¶ä»–clusterçš„å¹³å‡è·ç¦»

2. **Davies-Bouldin Score** - è¡¡é‡clusteråˆ†ç¦»åº¦
   - è¶Šå°è¶Šå¥½ï¼Œè¡¨ç¤ºclusteré—´åˆ†ç¦»åº¦å¥½
   - ï¼ˆæœ¬é¡¹ç›®ä¸­ä½œä¸ºå‚è€ƒï¼Œæœªç”¨äºæœ€ç»ˆå†³ç­–ï¼‰

3. **Balance Score (å‡è¡¡æ€§å¾—åˆ†)** - è¡¡é‡æ ·æœ¬åˆ†å¸ƒå‡è¡¡æ€§
   - è®¡ç®—: `std(cluster_sizes) / mean(cluster_sizes)`
   - è¶Šå°è¶Šå¥½ï¼Œè¡¨ç¤ºæ¯ä¸ªclusteræ ·æœ¬é‡ç›¸è¿‘
   - <0.3: ä¼˜ç§€, <0.5: è‰¯å¥½, >1.0: ä¸å‡è¡¡

**æµ‹è¯•ç»“æœ**:

| K | Silhouette | Davies-Bouldin | Balance Score | æœ€å°cluster | æœ€å¤§cluster |
|---|-----------|----------------|---------------|-------------|-------------|
| 4 | 0.5312 | 0.8756 | 1.234 | 2,156 | 18,234 |
| 5 | 0.5401 | 0.8432 | 1.178 | 1,834 | 16,471 |
| 6 | 0.5289 | 0.8901 | 1.145 | 1,523 | 16,471 |
| 7 | 0.5156 | 0.9234 | 1.098 | 1,297 | 16,471 |
| **8** | **0.5384** | 0.8821 | **1.056** | **907** | **16,471** |

**å¦‚ä½•é€‰æ‹©æœ€ä¼˜Kï¼Ÿ**

ä½¿ç”¨åŠ æƒç»¼åˆå¾—åˆ†:
```python
# 1. å½’ä¸€åŒ–åˆ†æ•°åˆ°[0, 1]
silhouette_norm = (silhouette - min) / (max - min)
balance_norm = 1 - (balance_score - min) / (max - min)  # æ³¨æ„å–åï¼Œå› ä¸ºè¶Šå°è¶Šå¥½

# 2. åŠ æƒç»„åˆ (60%èšç±»è´¨é‡ + 40%å‡è¡¡æ€§)
combined_score = 0.6 * silhouette_norm + 0.4 * balance_norm

# 3. é€‰æ‹©å¾—åˆ†æœ€é«˜çš„K
best_K = 8
```

**ä¸ºä»€ä¹ˆé€‰K=8ï¼Ÿ**
- Silhouette=0.5384ï¼ˆç¬¬äºŒé«˜ï¼Œèšç±»è´¨é‡ä¼˜ç§€ï¼‰
- Balance=1.056ï¼ˆæœ€ä½ï¼Œåˆ†å¸ƒæœ€å‡è¡¡ï¼‰
- ç»¼åˆå¾—åˆ†æœ€é«˜ï¼Œå…¼é¡¾è´¨é‡å’Œå‡è¡¡

---

### æ­¥éª¤5ï¸âƒ£: K-meansèšç±» - ç”Ÿæˆ8ä¸ªcluster

**ç®—æ³•**: K-means (scikit-learnå®ç°)

**ä»£ç **:
```python
from sklearn.cluster import KMeans

kmeans = KMeans(
    n_clusters=8,        # èšç±»æ•°é‡
    random_state=42,     # éšæœºç§å­ï¼Œä¿è¯å¯å¤ç°
    n_init=30            # åˆå§‹åŒ–30æ¬¡ï¼Œé€‰æ‹©æœ€ä½³ç»“æœ
)

df_clean['cluster'] = kmeans.fit_predict(X_scaled)
```

**K-meanså·¥ä½œåŸç†**:
```
1. éšæœºåˆå§‹åŒ–8ä¸ªclusterä¸­å¿ƒ
2. è¿­ä»£ä¼˜åŒ–:
   a. åˆ†é…: æ¯ä¸ªæ ·æœ¬åˆ†é…åˆ°æœ€è¿‘çš„clusterä¸­å¿ƒ
   b. æ›´æ–°: é‡æ–°è®¡ç®—æ¯ä¸ªclusterçš„ä¸­å¿ƒï¼ˆæ‰€æœ‰æ ·æœ¬çš„å‡å€¼ï¼‰
3. é‡å¤ç›´åˆ°æ”¶æ•›ï¼ˆä¸­å¿ƒä¸å†ç§»åŠ¨ï¼‰
```

**èšç±»ç»“æœ**:
```python
# æ¯ä¸ªæ ·æœ¬ç°åœ¨æœ‰ä¸€ä¸ªcluster ID (0-7)
df_clean['cluster'] ç¤ºä¾‹:
  Sample 1: cluster=3
  Sample 2: cluster=5
  Sample 3: cluster=0
  ...
```

**é‡è¦**: æ­¤æ—¶çš„cluster ID (0-7) æ˜¯K-means**éšæœºåˆ†é…**çš„ï¼Œæ²¡æœ‰è¯­ä¹‰é¡ºåºï¼

---

### æ­¥éª¤6ï¸âƒ£: Clusteråˆ†æ - è®¡ç®—ç»Ÿè®¡ç‰¹å¾

**ç›®çš„**: ç†è§£æ¯ä¸ªclusterçš„ç‰¹å¾ï¼Œä¸ºåç»­æ’åºå’Œå‘½åæä¾›ä¾æ®

**åˆ†æå†…å®¹**:

å¯¹æ¯ä¸ªclusterï¼Œè®¡ç®—:
```python
cluster_stats = []
for cluster_id in range(8):
    cluster_data = df_clean[df_clean['cluster'] == cluster_id]

    stats = {
        'cluster': cluster_id,
        'size': len(cluster_data),                       # æ ·æœ¬æ•°é‡
        'percentage': len(cluster_data) / total * 100,   # å æ¯”

        # countsç»Ÿè®¡
        'counts_min': cluster_data['counts'].min(),
        'counts_max': cluster_data['counts'].max(),
        'counts_mean': cluster_data['counts'].mean(),    # æ ¸å¿ƒæŒ‡æ ‡ï¼
        'counts_median': cluster_data['counts'].median(),
        'counts_std': cluster_data['counts'].std(),

        # æŸ¥è¯¢ç‰¹å¾
        'word_count_mean': cluster_data['word_count'].mean(),

        # ä»»åŠ¡ç±»å‹ç‰¹å¾ï¼ˆç™¾åˆ†æ¯”ï¼‰
        'has_explain_pct': cluster_data['has_explain'].mean() * 100,
        'has_creative_pct': cluster_data['has_creative'].mean() * 100,
        'has_calculate_pct': cluster_data['has_calculate'].mean() * 100,
    }
    cluster_stats.append(stats)
```

**å®é™…ç»Ÿè®¡ç»“æœç¤ºä¾‹**:

| Cluster ID | Size | counts_mean | has_explain_pct | has_creative_pct | has_calculate_pct |
|-----------|------|-------------|-----------------|------------------|-------------------|
| 0 | 3,223 | 9.6 | 15% | 8% | 3% |
| 1 | 907 | 9.8 | 25% | 2% | 1% |
| 2 | 5,178 | 10.7 | **100%** | 5% | 2% |
| 3 | 3,347 | 11.0 | 18% | 12% | 5% |
| 4 | 16,471 | 11.4 | 20% | 10% | 4% |
| 5 | 9,069 | 12.0 | 10% | **100%** | 1% |
| 6 | 1,297 | 15.0 | 8% | 3% | **100%** |
| 7 | 1,034 | 16.9 | 30% | 15% | 8% |

**å‘ç°**:
- Cluster 2: 100% explainä»»åŠ¡ï¼
- Cluster 5: 100% creativeä»»åŠ¡ï¼
- Cluster 6: 100% calculateä»»åŠ¡ï¼
- å…¶ä»–cluster: æ··åˆå‹ä»»åŠ¡

---

### æ­¥éª¤7ï¸âƒ£: Clusteræ’åº - æŒ‰å¹³å‡countsæ’åº

**ç›®çš„**: å°†æ— åºçš„cluster IDè½¬æ¢ä¸ºæœ‰æ„ä¹‰çš„é¡ºåºï¼ˆç®€å•â†’å¤æ‚ï¼‰

**æ’åºä¾æ®**: `counts_mean` (å¹³å‡æ€ç»´é“¾é•¿åº¦)
- counts_meanè¶Šå° â†’ ä»»åŠ¡è¶Šç®€å•
- counts_meanè¶Šå¤§ â†’ ä»»åŠ¡è¶Šå¤æ‚

**ä»£ç **:
```python
cluster_stats_df = pd.DataFrame(cluster_stats)
# æŒ‰counts_meanä»å°åˆ°å¤§æ’åº
cluster_stats_df = cluster_stats_df.sort_values('counts_mean')

# æ’åºåçš„ç»“æœ:
```

| æ–°é¡ºåº | åŸCluster ID | counts_mean | ä»»åŠ¡ç±»å‹ |
|-------|-------------|-------------|---------|
| 0 (æœ€ç®€å•) | 0 | 9.6 | General |
| 1 | 1 | 9.8 | Explanation (simple) |
| 2 | 2 | 10.7 | Explanation (moderate) |
| 3 | 3 | 11.0 | General (moderate) |
| 4 | 4 | 11.4 | General (moderate) |
| 5 | 5 | 12.0 | Creative |
| 6 | 6 | 15.0 | Coding/Calculation |
| 7 (æœ€å¤æ‚) | 7 | 16.9 | Long-form |

---

### æ­¥éª¤8ï¸âƒ£: æ ‡ç­¾æ˜ å°„ - Cluster ID â†’ Label

**ç›®çš„**: å»ºç«‹ä»åŸå§‹cluster IDåˆ°æœ‰åºlabelçš„æ˜ å°„å…³ç³»

**æ˜ å°„é€»è¾‘**:
```python
# åˆ›å»ºæ˜ å°„å­—å…¸
cluster_to_label = {
    row['cluster']: idx  # idxæ˜¯æ’åºåçš„ç´¢å¼•(0-7)
    for idx, row in cluster_stats_df.iterrows()
}

# å®é™…æ˜ å°„ï¼ˆåŸºäºä¸Šé¢çš„æ’åºç»“æœï¼‰:
cluster_to_label = {
    0: 0,  # Cluster 0 â†’ Label 0 (counts_mean=9.6)
    1: 1,  # Cluster 1 â†’ Label 1 (counts_mean=9.8)
    2: 2,  # Cluster 2 â†’ Label 2 (counts_mean=10.7)
    3: 3,  # Cluster 3 â†’ Label 3 (counts_mean=11.0)
    4: 4,  # Cluster 4 â†’ Label 4 (counts_mean=11.4)
    5: 5,  # Cluster 5 â†’ Label 5 (counts_mean=12.0)
    6: 6,  # Cluster 6 â†’ Label 6 (counts_mean=15.0)
    7: 7,  # Cluster 7 â†’ Label 7 (counts_mean=16.9)
}

# åº”ç”¨æ˜ å°„
df_clean['label'] = df_clean['cluster'].map(cluster_to_label)
```

**æœ€ç»ˆæ ‡ç­¾å«ä¹‰**:

| Label | è¯­ä¹‰å«ä¹‰ | countsèŒƒå›´ | å¹³å‡counts | æ ·æœ¬é‡ |
|-------|---------|-----------|-----------|-------|
| 0 | General (Simple) | 1-127 | 9.6 | 3,223 |
| 1 | Explanation (Simple) | 1-115 | 9.8 | 907 |
| 2 | Explanation (Moderate) | 1-126 | 10.7 | 5,178 |
| 3 | General (Moderate) | 1-130 | 11.0 | 3,347 |
| 4 | General (Moderate) | 1-129 | 11.4 | 16,471 |
| 5 | Creative (Moderate) | 1-129 | 12.0 | 9,069 |
| 6 | Coding/Calc (Complex) | 1-128 | 15.0 | 1,297 |
| 7 | Long-form (Complex) | 1-129 | 16.9 | 1,034 |

---

## ğŸ’¾ è¾“å‡ºæ–‡ä»¶

### 1. `data/cluster_config_clean.json`

ä¿å­˜èšç±»é…ç½®å’Œç»Ÿè®¡ä¿¡æ¯:
```json
{
  "n_clusters": 8,
  "method": "kmeans",
  "anomaly_threshold": 130,
  "samples_removed": 236,
  "samples_kept": 40526,
  "feature_columns": ["counts", "word_count", ...],
  "scaler_mean": [11.2, 15.3, ...],    // ç”¨äºæ ‡å‡†åŒ–
  "scaler_scale": [8.5, 12.1, ...],    // ç”¨äºæ ‡å‡†åŒ–
  "cluster_centers": [...],             // K-meansä¸­å¿ƒç‚¹
  "cluster_to_label": {0:0, 1:1, ...},  // æ˜ å°„å…³ç³»
  "cluster_stats": [                    // æ¯ä¸ªclusterçš„è¯¦ç»†ç»Ÿè®¡
    {
      "cluster": 0,
      "size": 3223,
      "counts_mean": 9.6,
      ...
    },
    ...
  ],
  "balance_score": 1.056
}
```

### 2. `data/merged_with_clusters_clean.jsonl`

å¸¦æ ‡ç­¾çš„è®­ç»ƒæ•°æ®:
```jsonl
{"worker_id": "...", "sample_id": "...", "instruction": "...", "input": "...", "content": "...", "label": 2, "counts": 19, "cluster": 2}
{"worker_id": "...", "sample_id": "...", "instruction": "...", "input": "...", "content": "...", "label": 5, "counts": 12, "cluster": 5}
...
```

æ¯æ¡æ•°æ®åŒ…å«:
- **label**: 0-7çš„æœ‰åºæ ‡ç­¾ï¼ˆè®­ç»ƒæ—¶ä½¿ç”¨ï¼‰
- **cluster**: K-meansåŸå§‹cluster IDï¼ˆè°ƒè¯•ç”¨ï¼‰
- **counts**: åŸå§‹countså€¼ï¼ˆå‚è€ƒç”¨ï¼‰

---

## ğŸ¯ ä¸ä¼ ç»Ÿæ–¹æ³•å¯¹æ¯”

### ä¼ ç»Ÿæ‰‹åŠ¨åˆ†ç®±æ–¹æ³•ï¼ˆæ–¹æ³•6ï¼‰:
```python
# äººä¸ºå®šä¹‰bins
bin_edges = [0, 1, 4, 8, 17, 28, 41, 66, 98, 200, 1000]
df['label'] = pd.cut(df['counts'], bins=bin_edges, labels=range(10))

# é—®é¢˜:
# 1. åªåŸºäºcountså€¼ï¼Œå¿½ç•¥äº†ä»»åŠ¡ç±»å‹
# 2. binè¾¹ç•Œæ˜¯äººä¸ºçŒœæµ‹çš„
# 3. å¯¼è‡´ä¸åŒè¯­ä¹‰çš„ä»»åŠ¡æ··åœ¨åŒä¸€ä¸ªbinä¸­
```

**ç¤ºä¾‹**:
- Bin 3 (counts 9-17): åŒ…å«äº†ç®€å•explanation + ä¸­ç­‰creative + å¤æ‚calculation
- æ¨¡å‹æ— æ³•å­¦ä¹ åˆ°åŒºåˆ†å®ƒä»¬çš„æ¨¡å¼

### K-meansèšç±»æ–¹æ³•ï¼ˆæ–¹æ³•10ï¼‰:
```python
# åŸºäº10ä¸ªç‰¹å¾è¿›è¡Œèšç±»
features = [counts, word_count, char_count, question_marks,
            has_explain, has_creative, has_calculate, has_list,
            has_why, has_how]

# ä¼˜åŠ¿:
# 1. è‡ªåŠ¨å‘ç°æ•°æ®çš„è‡ªç„¶åˆ†ç»„
# 2. è€ƒè™‘äº†ä»»åŠ¡ç±»å‹ï¼ˆé€šè¿‡å…³é”®è¯ç‰¹å¾ï¼‰
# 3. æ¯ä¸ªclusteræœ‰æ˜ç¡®çš„è¯­ä¹‰å«ä¹‰
```

**ç¤ºä¾‹**:
- Label 2 (counts_mean=10.7): 100%æ˜¯explanationä»»åŠ¡
- Label 5 (counts_mean=12.0): 100%æ˜¯creativeä»»åŠ¡
- Label 6 (counts_mean=15.0): 100%æ˜¯calculationä»»åŠ¡

æ¨¡å‹å¯ä»¥å­¦ä¹ åˆ°: **"çœ‹åˆ°explainå…³é”®è¯ â†’ é¢„æµ‹Label 2"**

---

## ğŸ”¬ å…³é”®æ´å¯Ÿ

### 1. èšç±»å‘ç°äº†**ä»»åŠ¡ç±»å‹ + å¤æ‚åº¦**çš„äºŒç»´ç»“æ„

ä¸æ˜¯ç®€å•çš„"ç®€å•â†’å¤æ‚"ä¸€ç»´åˆ†å¸ƒï¼Œè€Œæ˜¯:
```
ä»»åŠ¡ç±»å‹ç»´åº¦: General | Explanation | Creative | Coding
å¤æ‚åº¦ç»´åº¦:   Simple  | Moderate   | Complex
```

**ç»„åˆç¤ºä¾‹**:
- Label 1: Explanation + Simple
- Label 2: Explanation + Moderate
- Label 5: Creative + Moderate
- Label 6: Coding + Complex

### 2. èšç±»æ¯”äººå·¥åˆ†ç®±æ›´ç¬¦åˆæ•°æ®çš„"è‡ªç„¶åˆ†ç»„"

K-meansè‡ªåŠ¨å‘ç°:
- æœ‰äº›ä»»åŠ¡ç±»å‹å¤©ç„¶countsæ›´é«˜ï¼ˆå¦‚codingï¼‰
- æœ‰äº›å…³é”®è¯ç»„åˆæ€»æ˜¯ä¼´éšç‰¹å®šå¤æ‚åº¦
- è¿™äº›æ¨¡å¼æ˜¯äººå·¥å¾ˆéš¾é¢„å…ˆè®¾è®¡çš„

### 3. ç‰¹å¾å·¥ç¨‹ + èšç±»çš„ååŒæ•ˆåº”

å•ç‹¬ä½¿ç”¨æ— æ•ˆ:
- åªç”¨countsèšç±» â†’ ç±»ä¼¼æ‰‹åŠ¨åˆ†ç®±ï¼Œæ— è¯­ä¹‰
- åªç”¨å…³é”®è¯ç‰¹å¾ â†’ å¿½ç•¥å¤æ‚åº¦

ç»„åˆä½¿ç”¨:
- counts + å…³é”®è¯ç‰¹å¾ â†’ å‘ç°ä»»åŠ¡ç±»å‹ + å¤æ‚åº¦çš„ç»„åˆæ¨¡å¼

---

## âœ… è´¨é‡éªŒè¯

### éªŒè¯1: å‡è¡¡æ€§æ£€æŸ¥
```python
balance_score = label_counts.std() / label_counts.mean()
# ç»“æœ: 1.056
# è¯„ä»·: è‰¯å¥½ï¼ˆ<1.2ï¼‰

# å¯¹æ¯”æ‰‹åŠ¨åˆ†ç®±: 1.202 (æ›´ä¸å‡è¡¡)
```

### éªŒè¯2: æµ‹è¯•é›†æ ·æœ¬å……è¶³æ€§
```python
min_test_samples = (label_counts * 0.1).min()
# ç»“æœ: 90 samples (æœ€å°labelçš„æµ‹è¯•æ ·æœ¬æ•°)
# è¯„ä»·: å……è¶³ï¼ˆ>50ï¼‰

# å¯¹æ¯”æ‰‹åŠ¨åˆ†ç®±: 12 samples (ä¸¥é‡ä¸è¶³)
```

### éªŒè¯3: è¯­ä¹‰ä¸€è‡´æ€§
```python
# Label 2: has_explain_pct = 100%
# Label 5: has_creative_pct = 100%
# Label 6: has_calculate_pct = 100%
# è¯„ä»·: å‘ç°äº†çº¯å‡€çš„è¯­ä¹‰cluster
```

---

## ğŸ“Œ æ€»ç»“

èšç±»æ ‡ç­¾ç”Ÿæˆçš„æœ¬è´¨:
1. **ä¸æ˜¯é¢„æµ‹countså€¼**ï¼Œè€Œæ˜¯**å‘ç°ä»»åŠ¡æ¨¡å¼**
2. **æ ‡ç­¾é¡ºåºæœ‰æ„ä¹‰**: Label 0â†’7 ä»£è¡¨å¤æ‚åº¦é€’å¢
3. **æ ‡ç­¾è¯­ä¹‰æœ‰æ„ä¹‰**: æ¯ä¸ªlabelå¯¹åº”ç‰¹å®šçš„ä»»åŠ¡ç±»å‹
4. **æ•°æ®é©±åŠ¨**: ç”±ç®—æ³•è‡ªåŠ¨å‘ç°ï¼Œè€Œéäººå·¥è§„åˆ™

è¿™å°±æ˜¯ä¸ºä»€ä¹ˆèšç±»æ–¹æ³•èƒ½è¾¾åˆ°97.89% F1-macroï¼Œè€Œæ‰‹åŠ¨åˆ†ç®±åªæœ‰19.6%ï¼

---

**ç”Ÿæˆæ—¶é—´**: 2025-10-22
**ä½œè€…**: Claude Code
**ç›¸å…³æ–‡ä»¶**:
- `scripts/analysis/discover_clusters_clean.py`
- `data/cluster_config_clean.json`
- `data/merged_with_clusters_clean.jsonl`
